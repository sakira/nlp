{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Glove.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPeQFwd5pEY0WRx/JvCcn0q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakira/nlp/blob/main/Glove.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSldqFNeOkRk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7Vm8og6bLOH"
      },
      "source": [
        "#### Ref\n",
        "1. @inproceedings{pennington2014glove,\n",
        "  title={Glove: Global vectors for word representation},\n",
        "  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},\n",
        "  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},\n",
        "  pages={1532--1543},\n",
        "  year={2014}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGhDAhtTOmHL"
      },
      "source": [
        "### What is n-gram?\n",
        "Consider the word 'CATS'.\n",
        "You can break this word into individual letters such as 'C', 'A', 'T', 'S'\n",
        "Each letter in here is called 'token'.\n",
        "Or you can break down this word also like this 'CA', 'AT', 'TS'\n",
        "Each of these 'CA', 'AT', 'TS' are also called 'token'\n",
        "The first one is called 'unigram' 1-gram\n",
        "The second one is called 'bigram' 2-gram\n",
        "So on...\n",
        "\n",
        "Now, can you break down the sentence into 1-gram, 2-gram or 3-gram tokens of words?\n",
        "'I eat rice'\n",
        "'The quick brown fox jumps over the lazy dog.'\n",
        "\n",
        "Please note that there is a full stop in one of the examples. Think why or why not include it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiBY0sz6Qqbu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7RIlJvCQsWN"
      },
      "source": [
        "# You can also try to write a program which takes a string and the number of n as input. The output will be sequences of n-gram tokens.\n",
        "# For eg.\n",
        "# Input: cats, 2\n",
        "# Output: ca, at, ts\n",
        "# Input: I eat rice, 2\n",
        "# Output: \n",
        "# I eat\n",
        "# eat rice\n",
        "\n",
        "# Your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWCzITUtQQgI"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mvn7H-jQRj1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMxdAuLkQTBl"
      },
      "source": [
        "### How the n-gram related to Markov model?\n",
        "Please check what is Markov model means in the google.\n",
        "\n",
        "Now, given the word 'CATS' and the bi-gram tokens 'CA', 'AT, 'TS'. I am interested to know \n",
        "1. How often 'A' follows 'C' ie. P(A|C)\n",
        "2. How often 'T' follows 'A' ie. P(T|A)\n",
        "You can also read it like 'what is the probability of T given A'\n",
        "If you use counting based idea, then the probability of $x_t$ given $x_{t-1}$ can be computed as follows:\n",
        "\n",
        "p(x_t|x_{t-1}) = # of times 'CA' appears in the dataset 'CATS' / # of tims 'A' appears in the dataset 'CATS'\n",
        "\n",
        "Given the bangla alphabets. How many bigram probabilities we have to calculate?\n",
        "Can you write a program to do that?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX1Bs65XQR35"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VltK5PeVQR5O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6SK1j1MYtwQ"
      },
      "source": [
        "## Term-documents\n",
        "Now, here we will generate a 2-dimensional matrix.  Each row represents the number of words across all the documents and each column represents the number of words in each document. ie.\n",
        "\n",
        "              _ doc1 _ doc2 _ doc3 _\n",
        "      _ the   _ 2    _ 3    _ 0  _\n",
        "      _ apple _ 0    _ 1    _ 1  _\n",
        "      _ AI    _ 0    _ 10   _ 1  _\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Select four documents or paragraphs of your own choice from Google. Copy 10 lines. Save them in different files. Create a program that \n",
        "1. Reads from the file.\n",
        "2. Create a dictionary.\n",
        "3. Insert each unique word in the dictionary from Step 1.\n",
        "\n",
        "If you are done with creating dictionary, then do the following:\n",
        "1. Create a two dimensional matrix of size m and n, where m is the size of the dictionary you created earlier and n is the number of documents. In our case, it is 4. The value of each cell will be zero.\n",
        "2. Go through each document\n",
        "3. For each sentence in the document, if you find the word in the dictionary, increase the value of the cell to 1.\n",
        "4. Repeat step 2 and 3 until you are done.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNtcz-h8eIJZ"
      },
      "source": [
        " # put your code here\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ElD2q0heNRW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_O_NIUBRYtyV"
      },
      "source": [
        "Now we build word-word co occurances count matrix. We call it term-term matrix.\n",
        "We will use the previous ideas.\n",
        "1. Now, take 3/4 sentences.\n",
        "2. Create a dictionary of unique words from the step 1. so, this is a unigram token dictionary or vocabulary V.\n",
        "3. Create an empty(zeros) 2-d array, X  of size NxN, where N is the size of the dictionary.\n",
        "4. Now, we will fill up the values of X in such way:\n",
        "```\n",
        "I eat rice and dal.\n",
        "X('I', 'eat') = 1\n",
        "X('I', 'rice') = 1/2 (2 words away)\n",
        "X('I', 'dal') = 1/4 (4 words away)\n",
        "So on...\n",
        "```\n",
        "\n",
        "The following code will help to do this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2IIDnsqXJ86"
      },
      "source": [
        "for sentence in sentences:\n",
        "    n = len(sentence)\n",
        "    for i in range(n):\n",
        "        word = sentence[i] \n",
        "        wi = V(word) # get the word index from your vocabulary\n",
        "\n",
        "        # go through the words before i\n",
        "        for j in range(0, i):\n",
        "            wj = V(sentence[j])\n",
        "            scores = 1.0/(i-j)\n",
        "            X[wi, wj] += points\n",
        "            X[wj, wi] += points # since its symmetry\n",
        "\n",
        "        # go through the words after i\n",
        "        for j in range(i+1, n):\n",
        "            wj = V(sentence[j])\n",
        "            scores = 1.0/(j-i)\n",
        "            X[wi, wj] += points\n",
        "            X[wj, wi] += points # since its symmetry\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDQHzNnqcvvO"
      },
      "source": [
        "### X is the right hand side in Equation (3) of the above Ref 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIojCn4-dCVh"
      },
      "source": [
        "The following code is for Equation 9 in Ref 1.\n",
        "\n",
        "\n",
        "```\n",
        "fX = np.zeros_like(V)\n",
        "xmax = X.max()\n",
        "alpha = 3/4 # From the paper\n",
        "fX[X<xmax] = (X[X<xmax]/xmax)**alpha\n",
        "fX[X>=xmax] = 1\n",
        "logX = np.log(X+1) # Right hand side of Equation 7.\n",
        "\n",
        "```\n",
        "\n",
        "Costs in Equation 16. I omit the bias term.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def J(W, U, fX, logX):\n",
        "    delta = W.dot(U.T) - logX\n",
        "    return (fX*delta*delta).sum()\n",
        "```\n",
        "\n",
        "Now, we can do the training or update the parameter using the following code:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    cost = J(W, U, fX, logX)\n",
        "\n",
        "    # We update W, U for all values in V\n",
        "    oldW = W.copy()\n",
        "    for i in range(V):\n",
        "        W[i] -= learning_rate * dJdW(i, W, U, fX, logX)\n",
        "\n",
        "\n",
        "\n",
        "    for j in range(V):\n",
        "        U[j] -= learning_rate * dJdU(j, W, U, fX, logX)\n",
        "\n",
        "    # update the parameters\n",
        "    W = W - learning_rate * W\n",
        "    U = U - learning_rate * U\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "We need dJdW() and dJdU() functions which calculate the gradient with respect to W and U. The definition is as follows:\n",
        "```\n",
        "def dJdW(i, W, U, fX, logX):\n",
        "    return (fx[i, :]*delta[i, :]).dot(U)\n",
        "```\n",
        "\n",
        "```\n",
        "def dJdU(j, W, U, fX, logX):\n",
        "    return (fx[:, j]*delta[:, j]).dot(W)\n",
        "```\n",
        "\n",
        "This is the end of this tutorial.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHI0InREdBCv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVsp170vz_TB"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3KBRrQ_0BUc"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-qrjaguVp7m"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34FvnukZ0BXz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}